{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# notebook parameters\n",
    "\n",
    "import os\n",
    "\n",
    "spark_master = \"local[*]\"\n",
    "app_name = \"augment\"\n",
    "input_file = os.path.join(\"data\", \"WA_Fn-UseC_-Telco-Customer-Churn-.csv\")\n",
    "output_prefix = \"\"\n",
    "output_mode = \"overwrite\"\n",
    "output_kind = \"parquet\"\n",
    "driver_memory = '8g'\n",
    "executor_memory = '8g'\n",
    "\n",
    "dup_times = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity-checking\n",
    "\n",
    "We're going to make sure we're running with a compatible JVM first â€” if we run on macOS, we might get one that doesn't work with Scala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import getenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getenv(\"JAVA_HOME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = pyspark.sql.SparkSession.builder \\\n",
    "    .master(spark_master) \\\n",
    "    .appName(app_name) \\\n",
    "    .config(\"spark.driver.memory\", driver_memory) \\\n",
    "    .config(\"spark.executor.memory\", executor_memory) \\\n",
    "    .getOrCreate()\n",
    "session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schema definition\n",
    "\n",
    "Most of the fields are strings representing booleans or categoricals, but a few (`tenure`, `MonthlyCharges`, and `TotalCharges`) are numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "fields = [\"customerID\", \"gender\", \"SeniorCitizen\", \"Partner\", \"Dependents\", \"tenure\", \"PhoneService\", \"MultipleLines\", \"InternetService\", \"OnlineSecurity\", \"OnlineBackup\", \"DeviceProtection\", \"TechSupport\", \"StreamingTV\", \"StreamingMovies\", \"Contract\", \"PaperlessBilling\", \"PaymentMethod\", \"MonthlyCharges\", \"TotalCharges\", \"Churn\"]\n",
    "double_fields = set([\"tenure\", \"MonthlyCharges\", \"TotalCharges\"])\n",
    "\n",
    "schema = pyspark.sql.types.StructType(\n",
    "    [ pyspark.sql.types.StructField(f, DoubleType() if f in double_fields else StringType()) for f in fields ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = session.read.csv(input_file, header=True, schema=schema)\n",
    "\n",
    "original_df = original_df.dropna()\n",
    "pristine_df = original_df\n",
    "df = original_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_part(seed=0x5ca1ab1e):\n",
    "    \"generate the string part of a unique ID\"\n",
    "    import random\n",
    "    r = random.Random(seed)\n",
    "        \n",
    "    while True:\n",
    "        yield \"%X\" % r.getrandbits(24)\n",
    "\n",
    "sp = str_part()\n",
    "\n",
    "if dup_times > 1:\n",
    "    uniques = session.createDataFrame(schema = StructType(\n",
    "        [StructField(\"u_value\", StringType())]\n",
    "    ), data=[dict(u_value=next(sp)) for _ in range(dup_times)])\n",
    "    \n",
    "    original_df = original_df.crossJoin(uniques.distinct()) \\\n",
    "        .withColumn(\"customerID\",\n",
    "                    F.format_string(\n",
    "                        \"%s-%s\",\n",
    "                        \"customerID\",\n",
    "                        \"u_value\"\n",
    "                    )) \\\n",
    "        .drop(\"u_value\")\n",
    "    \n",
    "    df = original_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df.orderBy(\"customerID\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical and boolean features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"SeniorCitizen\", \n",
    "           \"Partner\", \n",
    "           \"Dependents\", \n",
    "           \"PhoneService\", \n",
    "           \"MultipleLines\", \n",
    "           \"InternetService\", \n",
    "           \"OnlineSecurity\", \n",
    "           \"OnlineBackup\", \n",
    "           \"DeviceProtection\", \n",
    "           \"TechSupport\", \n",
    "           \"StreamingTV\", \n",
    "           \"StreamingMovies\",\n",
    "           \"Contract\",\n",
    "           \"PaperlessBilling\",\n",
    "           \"PaymentMethod\"]\n",
    "\n",
    "if False:\n",
    "    for c in columns:\n",
    "        print(c, [row[0] for row in original_df.select(c).distinct().rdd.collect()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the data frame\n",
    "\n",
    "The training data schema looks like this:\n",
    "\n",
    "- customerID\n",
    "- gender\n",
    "- SeniorCitizen\n",
    "- Partner\n",
    "- Dependents\n",
    "- tenure\n",
    "- PhoneService\n",
    "- MultipleLines\n",
    "- InternetService\n",
    "- OnlineSecurity\n",
    "- OnlineBackup\n",
    "- DeviceProtection\n",
    "- TechSupport\n",
    "- StreamingTV\n",
    "- StreamingMovies\n",
    "- Contract\n",
    "- PaperlessBilling\n",
    "- PaymentMethod\n",
    "- MonthlyCharges\n",
    "- TotalCharges\n",
    "- Churn\n",
    "\n",
    "We want to divide the data frame into several frames that we can join together in an ETL job.\n",
    "\n",
    "Those frames will look like this:\n",
    "\n",
    "- **Customer metadata**\n",
    "  - customerID\n",
    "  - gender\n",
    "  - date of birth (we'll derive age and senior citizen status from this)\n",
    "  - Partner\n",
    "  - Dependents\n",
    "  - (nominal) MonthlyCharges\n",
    "- **Billing events**\n",
    "  - customerID\n",
    "  - date (we'll derive tenure from the number/duration of billing events)\n",
    "  - kind (one of \"AccountCreation\", \"Charge\", or \"AccountTermination\")\n",
    "  - value (either a positive nonzero amount or 0.00; we'll derive TotalCharges from the sum of amounts and Churn from the existence of an AccountTermination event)\n",
    "- **Customer phone features**\n",
    "  - customerID\n",
    "  - feature (one of \"PhoneService\" or \"MultipleLines\")\n",
    "- **Customer internet features**\n",
    "  - customerID\n",
    "  - feature (one of \"InternetService\", \"OnlineSecurity\", \"OnlineBackup\", \"DeviceProtection\", \"TechSupport\", \"StreamingTV\", \"StreamingMovies\")\n",
    "  - value (one of \"Fiber\", \"DSL\", \"Yes\", \"No\")\n",
    "- **Customer account features**\n",
    "  - customerID\n",
    "  - feature (one of \"Contract\", \"PaperlessBilling\", \"PaymentMethod\")\n",
    "  - value (one of \"Month-to-month\", \"One year\", \"Two year\", \"No\", \"Yes\", \"Credit card (automatic)\", \"Mailed check\", \"Bank transfer (automatic)\", \"Electronic check\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by generating a series of monthly charges (in the `charges` data frame), then a series of account creation events (`serviceStarts`) and churn events (`serviceTerminations`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "now = datetime.datetime.now(datetime.timezone.utc)\n",
    "\n",
    "w = pyspark.sql.Window.orderBy(F.lit('')).partitionBy(df.customerID)\n",
    "\n",
    "charges = df.select(\n",
    "    df.customerID, \n",
    "    F.lit(\"Charge\").alias(\"kind\"),\n",
    "    F.explode(F.array_repeat(df.TotalCharges / df.tenure, df.tenure.cast(\"int\"))).alias(\"value\")\n",
    ").withColumn(\"now\", F.lit(now)\n",
    ").withColumn(\"month_number\", -F.row_number().over(w)\n",
    ").withColumn(\"date\", F.expr(\"add_months(now, month_number)\")).drop(\n",
    "    \"now\", \"month_number\"\n",
    ")\n",
    "\n",
    "serviceStarts = df.select(\n",
    "    df.customerID,\n",
    "    F.lit(\"AccountCreation\").alias(\"kind\"),\n",
    "    F.lit(0.0).alias(\"value\"),\n",
    "    F.lit(now).alias(\"now\"),\n",
    "    (-df.tenure - 1).alias(\"month_number\")\n",
    ").withColumn(\"date\", F.expr(\"add_months(now, month_number)\")).drop(\n",
    "    \"now\", \"month_number\"\n",
    ")\n",
    "\n",
    "serviceTerminations = df.where(df.Churn == \"Yes\").select(\n",
    "    df.customerID,\n",
    "    F.lit(\"AccountTermination\").alias(\"kind\"),\n",
    "    F.lit(0.0).alias(\"value\"),\n",
    "    F.add_months(F.lit(now), 0).alias(\"date\")\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`billingEvents` is the data frame containing all of these events:  account activation, account termination, and individual payment events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "billingEvents = charges.union(serviceStarts).union(serviceTerminations).orderBy(\"date\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a little helper function to use the parameters we defined earlier while writing data frames to Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_df(df, name):\n",
    "    name = \"%s.%s\" % (name, output_kind)\n",
    "    if output_prefix != \"\":\n",
    "        name = \"%s-%s\" % (output_prefix, name)\n",
    "    kwargs = {}\n",
    "    if output_kind == \"csv\":\n",
    "        kwargs['header'] = True\n",
    "    getattr(df.write.mode(output_mode), output_kind)(name, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_df(billingEvents, \"billing_events\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step is to generate customer metadata, which includes the following fields:\n",
    "\n",
    "  - gender\n",
    "  - date of birth (we'll derive age and senior citizen status from this)\n",
    "  - Partner\n",
    "  - Dependents\n",
    "  \n",
    "We'll calculate date of birth by using the hash of the customer ID as a pseudorandom number and then assuming that ages are uniformly distributed between 18-65 and exponentially distributed over 65."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENIOR_CUTOFF = 65\n",
    "ADULT_CUTOFF = 18\n",
    "DAYS_IN_YEAR = 365.25\n",
    "EXPONENTIAL_DIST_SCALE = 6.3\n",
    "\n",
    "customerMetaRaw = original_df.select(\n",
    "    \"customerID\",                \n",
    "    F.lit(now).alias(\"now\"),\n",
    "    (F.abs(F.hash(original_df.customerID)) % 4096 / 4096).alias(\"choice\"),\n",
    "    \"SeniorCitizen\",\n",
    "    \"gender\",\n",
    "    \"Partner\",\n",
    "    \"Dependents\",\n",
    "    \"MonthlyCharges\"\n",
    ")\n",
    "\n",
    "customerMetaRaw = customerMetaRaw.withColumn(\n",
    "    \"ageInDays\",\n",
    "    F.floor(\n",
    "        F.when(\n",
    "            customerMetaRaw.SeniorCitizen == 0, \n",
    "            (customerMetaRaw.choice * \n",
    "             ((SENIOR_CUTOFF - ADULT_CUTOFF - 1) * DAYS_IN_YEAR)) \n",
    "            + (ADULT_CUTOFF * DAYS_IN_YEAR)\n",
    "        ).otherwise(\n",
    "            (SENIOR_CUTOFF * DAYS_IN_YEAR) + \n",
    "            (DAYS_IN_YEAR * (-F.log1p(-customerMetaRaw.choice) * EXPONENTIAL_DIST_SCALE))\n",
    "        )\n",
    "    ).cast(\"int\")\n",
    ")\n",
    "\n",
    "customerMetaRaw = customerMetaRaw.withColumn(\n",
    "    \"dateOfBirth\",\n",
    "    F.expr(\"date_sub(now, ageInDays)\")\n",
    ")\n",
    "\n",
    "customerMeta = customerMetaRaw.select(\n",
    "    \"customerID\",\n",
    "    \"dateOfBirth\",\n",
    "    \"gender\",\n",
    "    \"SeniorCitizen\",\n",
    "    \"Partner\",\n",
    "    \"Dependents\",\n",
    "    \"MonthlyCharges\",\n",
    "    \"now\"\n",
    ").orderBy(\"customerID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_df(customerMeta, \"customer_meta\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can generate customer phone features, which include:\n",
    "\n",
    "  - customerID\n",
    "  - feature (one of \"PhoneService\" or \"MultipleLines\")\n",
    "  - value (always \"Yes\"; there are no records for \"No\" or \"No Phone Service\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phoneService = original_df.select(\n",
    "    \"customerID\",\n",
    "    F.lit(\"PhoneService\").alias(\"feature\"),\n",
    "    F.lit(\"Yes\").alias(\"value\")\n",
    ").where(original_df.PhoneService == \"Yes\")\n",
    "\n",
    "multipleLines = original_df.select(\n",
    "    \"customerID\",\n",
    "    F.lit(\"MultipleLines\").alias(\"feature\"),\n",
    "    F.lit(\"Yes\").alias(\"value\")\n",
    ").where(original_df.MultipleLines == \"Yes\")\n",
    "\n",
    "customerPhoneFeatures = phoneService.union(multipleLines).orderBy(\"customerID\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_df(customerPhoneFeatures, \"customer_phone_features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customer internet features include:\n",
    "  - customerID\n",
    "  - feature (one of \"InternetService\", \"OnlineSecurity\", \"OnlineBackup\", \"DeviceProtection\", \"TechSupport\", \"StreamingTV\", \"StreamingMovies\")\n",
    "  - value (one of \"Fiber\", \"DSL\", \"Yes\" -- no records for \"No\" or \"No internet service\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "internet_service = original_df.select(\n",
    "    \"customerID\",\n",
    "    F.lit(\"InternetService\").alias(\"feature\"),\n",
    "    original_df.InternetService.alias(\"value\")\n",
    ").where(original_df.InternetService != \"No\")\n",
    "\n",
    "customerInternetFeatures = internet_service\n",
    "\n",
    "for feature in [\"InternetService\", \"OnlineSecurity\", \"OnlineBackup\", \"DeviceProtection\", \"TechSupport\", \"StreamingTV\", \"StreamingMovies\"]:\n",
    "    tmpdf = original_df.select(\n",
    "        \"customerID\",\n",
    "        F.lit(feature).alias(\"feature\"),\n",
    "        original_df[feature].alias(\"value\")\n",
    "    ).where(original_df[feature] == \"Yes\")\n",
    "    \n",
    "    customerInternetFeatures = customerInternetFeatures.union(tmpdf)\n",
    "\n",
    "write_df(customerInternetFeatures.orderBy(\"customerID\"), \"customer_internet_features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customer account features include:\n",
    "\n",
    "  - customerID\n",
    "  - feature (one of \"Contract\", \"PaperlessBilling\", \"PaymentMethod\")\n",
    "  - value (one of \"Month-to-month\", \"One year\", \"Two year\", \"Yes\", \"Credit card (automatic)\", \"Mailed check\", \"Bank transfer (automatic)\", \"Electronic check\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accountSchema = pyspark.sql.types.StructType(\n",
    "    [ pyspark.sql.types.StructField(f, StringType()) for f in [\"customerID\", \"feature\", \"value\"]]\n",
    ")\n",
    "\n",
    "customerAccountFeatures = session.createDataFrame(schema=accountSchema, data=[])\n",
    "\n",
    "for feature in [\"Contract\", \"PaperlessBilling\", \"PaymentMethod\"]:\n",
    "    tmpdf = original_df.select(\n",
    "        \"customerID\",\n",
    "        F.lit(feature).alias(\"feature\"),\n",
    "        original_df[feature].alias(\"value\")\n",
    "    ).where(original_df[feature] != \"No\")\n",
    "    \n",
    "    customerAccountFeatures = customerAccountFeatures.union(tmpdf)\n",
    "    \n",
    "write_df(customerAccountFeatures, \"customer_account_features\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
